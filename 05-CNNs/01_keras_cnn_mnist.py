# -*- coding: utf-8 -*-
"""01-Keras-CNN-MNIST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kQWLakaMgXu_CNIbOWyMCkP75xGWOReq

<strong><center>Copyright by Pierian Data Inc.</center></strong> 
<strong><center>Created by Jose Marcial Portilla.</center></strong>
# Convolutional Neural Networks for Image Classification
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from tensorflow.keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

"""##  Visualizing the Image Data"""

x_train.shape

single_image = x_train[0]

single_image

# 28x28 pixels
single_image.shape

plt.imshow(single_image, cmap='gray')
plt.show()

"""# PreProcessing Data

We first need to make sure the labels will be understandable by our CNN.

## Labels
"""

y_train

y_test

"""Our labels are literally categories of numbers. We need to translate this to be "one hot encoded" so our CNN can understand, otherwise it will think this is some sort of regression problem on a continuous axis. Keras has an easy to use function for this:"""

from tensorflow.keras.utils import to_categorical

y_example = to_categorical(y_train)

y_example

y_example.shape

y_example[0]

y_cat_test = to_categorical(y_test, 10)

y_cat_train = to_categorical(y_train, 10)

"""### Processing X Data

We should normalize the X data
"""

single_image.max()

single_image.min()

x_train = x_train/255
x_test = x_test/255

scaled_image = x_train[0]

scaled_image.max()

scaled_image.min()

scaled_image

plt.imshow(scaled_image, cmap='Purples')
plt.show()

"""## Reshaping the Data

Right now our data is 60,000 images stored in 28 by 28 pixel array formation. 

This is correct for a CNN, but we need to add one more dimension to show we're dealing with 1 RGB channel (since technically the images are in black and white, only showing values from 0-255 on a single channel), an color image would have 3 dimensions.
"""

x_train.shape

x_test.shape

"""Reshape to include channel dimension (in this case, 1 channel)"""

# batch_size, width, height, color_channels
x_train = x_train.reshape(60000,28,28,1)

# batch_size, width, height, color_channels
x_test = x_test.reshape(10000,28,28,1)

"""# Training the Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten

model = Sequential()

# CONVOLUTIONAL LAYER
model.add(Conv2D(filters=32, kernel_size=(4,4), input_shape=(28, 28, 1), activation='relu')) # filters almost always 2^n
# POOLING LAYER
model.add(MaxPool2D(pool_size=(2,2))) # in this case, half of the kernel size

# FLATTEN IMAGES FROM 28 by 28 to 764 BEFORE FINAL LAYER
model.add(Flatten())

# 128 NEURONS IN DENSE HIDDEN LAYER (YOU CAN CHANGE THIS NUMBER OF NEURONS)
model.add(Dense(128, activation='relu'))

# LAST LAYER IS THE CLASSIFIER, THUS 10 POSSIBLE CLASSES
model.add(Dense(10, activation='softmax'))  # SOFTMAX --> MULTICLASS PROBLEM

# https://keras.io/metrics/
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics = ['accuracy'])

model.summary()

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='val_loss', patience=1)

"""## Train the Model"""

model.fit(x_train, y_cat_train, epochs=10, validation_data=(x_test, y_cat_test), callbacks=[early_stop])

"""## Evaluate the Model"""

model.metrics_names

metrics = pd.DataFrame(model.history.history)

metrics

metrics[['loss', 'val_loss']].plot()
plt.show()

metrics[['accuracy', 'val_accuracy']].plot()
plt.show()

print(model.metrics_names)
print(model.evaluate(x_test, y_cat_test, verbose = 0))

from sklearn.metrics import classification_report, confusion_matrix

predictions = model.predict_classes(x_test)

predictions_2 = np.argmax(model.predict(x_test), axis=-1)

predictions[0]

predictions_2[0]

y_cat_test[0]

y_test[0]

print(classification_report(y_test, predictions))

print(classification_report(y_test, predictions_2))

print(confusion_matrix(y_test,predictions))

print(confusion_matrix(y_test,predictions_2))

import seaborn as sns

plt.figure(figsize=(10,7))
sns.heatmap(confusion_matrix(y_test, predictions_2), annot=True)
plt.show()

"""# Predicting a given image"""

my_number = x_test[4]

plt.imshow(my_number.reshape(28,28), cmap='Greys')
plt.show()

# SHAPE --> (num_images,width,height,color_channels)
print(np.argmax(model.predict(my_number.reshape(1,28,28,1)), axis=-1))

"""## Saving the Model"""

model.save('mnist_CNN_model.h5')