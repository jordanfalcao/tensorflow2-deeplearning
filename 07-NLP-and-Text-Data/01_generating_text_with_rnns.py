# -*- coding: utf-8 -*-
"""01_Generating_Text_with_RNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1okrBSdijOPNiJWU14vjd5FVs017T7_Mk

# Text Generation with Neural Networks

In this notebook we will create a network that can generate text, here we show it being done character by character. Very awesome write up on this here: http://karpathy.github.io/2015/05/21/rnn-effectiveness/

We organized the process into "steps" so you can easily follow along with your own data sets.
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf

tf.__version__

print("Num GPUs Available: ", len(tf.config.list_physical_devices('GPU')))

"""## Step 1: The Data

You can grab any free text you want from here: https://www.gutenberg.org/

We'll choose all of shakespeare's works (which we have already downloaded for you), mainly for two reasons:

1. Its a large corpus of text, its usually recommended you have at least a source of 1 million characters total to get realistic text generation.

2. It has a very distinctive style. Since the text data uses old style english and is formatted in the style of a stage play, it will be very obvious to us if the model is able to reproduce similar results.
"""

text = open('shakespeare.txt', 'r').read()

type(text)

len(text)

print(text[:667])

"""### Understanding unique characters"""

# The unique characters in the file
vocab = sorted(set(text))
print(vocab)
len(vocab)

"""## Step 2: Text Processing

### Text Vectorization

We know a neural network can't take in the raw string data, we need to assign numbers to each character. Let's create two dictionaries that can go from numeric index to character and character to numeric index.
"""

# creating a dict
char_to_ind = {char:ind for ind, char in enumerate(vocab)}

char_to_ind

ind_to_char = np.array(vocab)

ind_to_char

encoded_text = np.array([char_to_ind[c] for c in text])

"""We now have a mapping we can use to go back and forth from characters to numerics."""

encoded_text[:667]

"""## Step 3: Creating Batches

Overall what we are trying to achieve is to have the model predict the next highest probability character given a historical sequence of characters. Its up to us (the user) to choose how long that historic sequence. Too short a sequence and we don't have enough information (e.g. given the letter "a" , what is the next character) , too long a sequence and training will take too long and most likely overfit to sequence characters that are irrelevant to characters farther out. While there is no correct sequence length choice, you should consider the text itself, how long normal phrases are in it, and a reasonable idea of what characters/words are relevant to each other.
"""

print(text[:500])

# first line
line = 'From fairest creatures we desire increase,'

len(line)

# catching the pattern
part_stanza = """From fairest creatures we desire increase,
  That thereby beauty's rose might never die,
  But as the riper should by time decease,"""

len(part_stanza)

"""### Training Sequences

The actual text data will be the text sequence shifted one character forward. For example:

**Sequence In: "Hello my nam"**

**Sequence Out: "ello my name"**


We can use the `tf.data.Dataset.from_tensor_slices` function to convert a text vector into a stream of character indices (dataset).
"""

seq_len = 120

# +1: avoid ind 0 
total_num_seq = len(text)//(seq_len+1)

total_num_seq

# Create Training Sequences (dataset)
char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)

# example
for i in char_dataset.take(50):
     print(i.numpy())

# to char - example
for i in char_dataset.take(50):
     print(ind_to_char[i.numpy()])

"""The **batch** method converts these individual character calls into sequences we can feed in as a batch. We use seq_len+1 because of zero indexing. Here is what drop_remainder means:

drop_remainder: (Optional.) A `tf.bool` scalar `tf.Tensor`, representing
    whether the last batch should be dropped in the case it has fewer than
    `batch_size` elements; the default behavior is not to drop the smaller
    batch.

"""

sequences = char_dataset.batch(batch_size=seq_len+1, drop_remainder=True)

"""Now that we have our sequences, we will perform the following steps for each one to create our target text sequences:

1. Grab the input text sequence
2. Assign the target text sequence as the input text sequence shifted by one step forward
3. Group them together as a tuple
"""

def create_seq_targets(seq):
    input_txt = seq[:-1] # grab input sequence
    target_txt = seq[1:] # shift 1
    return input_txt, target_txt # return a tuple

dataset = sequences.map(create_seq_targets)

# example
for input_txt, target_txt in dataset.take(1):
    print(input_txt.numpy())
    print(''.join(ind_to_char[input_txt.numpy()]))
    print('\n')

    print(target_txt.numpy())
    print(''.join(ind_to_char[target_txt.numpy()]))     # There is an extra whitespace at the end!

"""### Generating training batches

Now that we have the actual sequences, we will create the batches, we want to shuffle these sequences into a random order, so the model doesn't overfit to any section of the text, but can instead generate characters given any seed text.
"""

# Batch size - 128 sequences per time
batch_size = 128

# Buffer size to shuffle the dataset so it doesn't attempt to shuffle the
# entire sequence in memory. Instead, it maintains a buffer in which it shuffles elements
buffer_size = 10000

dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

# (input text), (target text), 128 sequences of length 120
dataset

"""## Step 4: Creating the Model

We will use an LSTM based model with a few extra features, including an embedding layer to start off with and **two** LSTM layers. We based this model architecture off the [DeepMoji](https://deepmoji.mit.edu/) and the original source code can be found [here](https://github.com/bfelbo/DeepMoji).

The embedding layer will serve as the input layer, which essentially creates a lookup table that maps the numbers indices of each character to a vector with "embedding dim" number of dimensions. As you can imagine, the larger this embedding size, the more complex the training. This is similar to the idea behind word2vec, where words are mapped to some n-dimensional space. Embedding before feeding straight into the LSTM usually leads to more realisitic results.
"""

# Length of the vocabulary in chars
vocab_size = len(vocab)

# The embedding dimension - more or less the same range as the vocab size
embed_dim = 64

# Number of RNN units
rnn_neurons = 1026

"""Now let's create a function that easily adapts to different variables as shown above.

### Setting up Loss Function

For our loss we will use sparse categorical crossentropy, which we can import from Keras. We will also set this as logits=True
"""

from tensorflow.keras.losses import sparse_categorical_crossentropy

# help(sparse_categorical_crossentropy)

"""https://datascience.stackexchange.com/questions/41921/sparse-categorical-crossentropy-vs-categorical-crossentropy-keras-accuracy"""

# custom loss function - just setting 'from_logits=True'
def sparse_cat_loss(y_true,y_pred):
  return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True) # True because is hot encoded

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout, GRU

def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):
    model = Sequential()

    # ebbedding layer
    model.add(Embedding(vocab_size, embed_dim,batch_input_shape=[batch_size, None]))

    # GRU layer
    model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer='glorot_uniform'))

    # Final Dense Layer to Predict
    model.add(Dense(vocab_size))

    # this is why we custom our loss fuction, we couldn't take 'from_logits=True'
    model.compile(optimizer='adam', loss=sparse_cat_loss) 
    return model

model = create_model(
  vocab_size = vocab_size,
  embed_dim=embed_dim,
  rnn_neurons=rnn_neurons,
  batch_size=batch_size)

# over 3 million parameters
model.summary()

"""## Step 5: Training the model

Let's make sure everything is ok with our model before we spend too much time training! Let's pass in a batch to confirm the model currently predicts random characters without any training.

"""

for input_example_batch, target_example_batch in dataset.take(1):

  # Predict off some random batch
  example_batch_predictions = model(input_example_batch)

# Display the dimensions of the predictions
print(example_batch_predictions.shape, " <=== (batch_size, sequence_length, vocab_size)")

# bunch of probabilities
example_batch_predictions[0]

sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)

sampled_indices

# Reformat to not be a lists of lists
sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()

sampled_indices

ind_to_char[sampled_indices]

print("Given the input seq: \n")
print("".join(ind_to_char[input_example_batch[0]]))
print('\n')
print("Next Char Predictions: \n")
print("".join(ind_to_char[sampled_indices ]))

"""### Train our network!"""

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(monitor='loss', patience=2)

model.fit(dataset,epochs=30, callbacks=[early_stop])

pd.DataFrame(model.history.history).plot(figsize=(8,5))
plt.show()

"""### Saving model"""

model.save('shakespeare_text_gen.h5')

"""## Evaluating the model"""

from tensorflow.keras.models import load_model

# importing the model in a different way
model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)

model.load_weights('shakespeare_text_gen.h5')

model.build(tf.TensorShape([1, None]))

def generate_text(model, start_seed,gen_size=100,temp=1.0):
  '''
  model: Trained Model to Generate Text
  start_seed: Intial Seed text in string form
  gen_size: Number of characters to generate

  Basic idea behind this function is to take in some seed text, format it so
  that it is in the correct shape for our network, then loop the sequence as
  we keep adding our own predicted characters. Similar to our work in the RNN
  time series problems.
  '''

  # Number of characters to generate
  num_generate = gen_size

  # Vecotrizing starting seed text
  input_eval = [char_to_ind[s] for s in start_seed]

  # Expand to match batch format shape
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty list to hold resulting generated text
  text_generated = []

  # Temperature effects randomness in our resulting text
  # The term is derived from entropy/thermodynamics.
  # The temperature is used to effect probability of next characters.
  # Higher probability == lesss surprising/ more expected
  # Lower temperature == more surprising / less expected
 
  temperature = temp

  # Here batch size == 1
  model.reset_states()

  for i in range(num_generate):

      # Generate Predictions
      predictions = model(input_eval)

      # Remove the batch shape dimension
      predictions = tf.squeeze(predictions, 0)

      # Use a cateogircal disitribution to select the next character
      predictions = predictions / temperature
      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

      # Pass the predicted charracter for the next input
      input_eval = tf.expand_dims([predicted_id], 0)

      # Transform back to character letter
      text_generated.append(ind_to_char[predicted_id])

  return (start_seed + ''.join(text_generated))

print(generate_text(model,"JULIET",gen_size=500))

print(generate_text(model,"From fairest creatures",gen_size=500))