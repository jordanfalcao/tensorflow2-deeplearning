# -*- coding: utf-8 -*-
"""01-Generative-Adversarial-Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2kOCVHivmBcKfprPk_436y0rCArQ4iP

# GANs - Generative Adverserial Networks
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

plt.imshow(X_train[0])
plt.show()

y_train

"""## Filtering out the Data for Faster Training on Smaller Dataset"""

only_zeros = X_train[y_train==0]

only_zeros.shape

plt.imshow(only_zeros[950])
plt.show()

"""## Create the model"""

from tensorflow.keras.layers import Dense, Reshape, Flatten
from tensorflow.keras.models import Sequential

"""We compile the discriminator, but we don't compile the generator."""

discriminator = Sequential()

discriminator.add(Flatten(input_shape=[28,28]))
discriminator.add(Dense(150, activation='relu'))
discriminator.add(Dense(100, activation='relu'))

# final output layer
discriminator.add(Dense(1, activation='sigmoid')) # real or fake - 0 or 1

discriminator.compile(loss='binary_crossentropy', optimizer='adam')

# we don't compile the generator, because it's only trained through the full GAN model
codings_size = 100

# similar as autoencoder: 784 --> 150 --> 100 --> 150 --> 784
generator = Sequential()

generator.add(Dense(100, activation='relu', input_shape=[codings_size]))
generator.add(Dense(150, activation='relu'))
generator.add(Dense(784, activation='relu'))

generator.add(Reshape([28,28]))

GAN = Sequential([generator, discriminator])

# the discriminator shouldn't be trained during the second phase
discriminator.trainable = False

# layers
discriminator.layers[0].trainable

# hidden layer
discriminator.layers[1].trainable

GAN.compile(loss='binary_crossentropy', optimizer='adam')

# two sequential models
GAN.layers

GAN.summary()

# generator
GAN.layers[0].summary()

# discriminator
GAN.layers[1].summary()

"""### Setting up Training Batches"""

batch_size = 32

"""https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle

The buffer_size in Dataset.shuffle() can affect the randomness of your dataset, and hence the order in which elements are produced. 
"""

# change data, if you want
# my_data = X_train
my_data = only_zeros

dataset = tf.data.Dataset.from_tensor_slices(my_data).shuffle(buffer_size=1000)

type(dataset)

my_data.shape

# drop_remainder=True, because 5923/32 != 0
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)

epochs = 1

"""**NOTE: The generator never actually sees any real images. It learns by viewing the gradients going back through the discriminator. The better the discrimnator gets through training, the more information the discriminator contains in its gradients, which means the generator can being to make progress in learning how to generate fake images, in our case, fake zeros.**

## Training Loop
"""

# Grab the seprate components
generator, discriminator = GAN.layers

# For every epcoh
for epoch in range(epochs):
    print(f"Currently on Epoch {epoch+1}")
    i = 0
    # For every batch in the dataset
    for X_batch in dataset:
        i=i+1
        if i%100 == 0:
            print(f"\tCurrently on batch number {i} of {len(my_data)//batch_size}")
        #####################################
        ## TRAINING THE DISCRIMINATOR ######
        ###################################
        
        # Create Noise
        noise = tf.random.normal(shape=[batch_size, codings_size])
        
        # Generate numbers based just on noise input
        gen_images = generator(noise)
        
        # Concatenate Generated Images against the Real Ones
        # TO use tf.concat, the data types must match!
        X_fake_vs_real = tf.concat([gen_images, tf.dtypes.cast(X_batch,tf.float32)], axis=0)
        
        # Targets set to zero for fake images and 1 for real images
        y1 = tf.constant([[0.]] * batch_size + [[1.]] * batch_size)
        
        # This gets rid of a Keras warning
        discriminator.trainable = True
        
        # Train the discriminator on this batch
        discriminator.train_on_batch(X_fake_vs_real, y1)
        
        
        #####################################
        ## TRAINING THE GENERATOR     ######
        ###################################
        
        # Create some noise
        noise = tf.random.normal(shape=[batch_size, codings_size])
        
        # We want discriminator to belive that fake images are real
        y2 = tf.constant([[1.]] * batch_size)
        
        # Avois a warning
        discriminator.trainable = False
        
        GAN.train_on_batch(noise, y2)
        
print("TRAINING COMPLETE")

noise = tf.random.normal(shape=[10, codings_size])

noise.shape

plt.imshow(noise)
plt.show()

image = generator(noise)

plt.imshow(image[1])
plt.show()

plt.imshow(image[3])
plt.show()

"""## Saving models"""

generator.save('mnist_GAN_generator.h5')

discriminator.save('mnist_GAN_discriminator.h5')

GAN.save('mnist_GAN_model.h5')

