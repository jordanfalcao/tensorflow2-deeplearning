# -*- coding: utf-8 -*-
"""01-Generative-Adversarial-Networks.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K2kOCVHivmBcKfprPk_436y0rCArQ4iP

# GANs - Generative Adverserial Networks
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from tensorflow.keras.datasets import mnist

(X_train, y_train), (X_test, y_test) = mnist.load_data()

plt.imshow(X_train[0])
plt.show()

y_train

"""## Filtering out the Data for Faster Training on Smaller Dataset"""

only_zeros = X_train[y_train==0]

only_zeros.shape

plt.imshow(only_zeros[950])
plt.show()

"""## Create the model"""

from tensorflow.keras.layers import Dense, Reshape, Flatten
from tensorflow.keras.models import Sequential

"""We compile the discriminator, but we don't compile the generator."""

discriminator = Sequential()

discriminator.add(Flatten(input_shape=[28,28]))
discriminator.add(Dense(150, activation='relu'))
discriminator.add(Dense(100, activation='relu'))

# final output layer
discriminator.add(Dense(1, activation='sigmoid')) # real or fake - 0 or 1

discriminator.compile(loss='binary_crossentropy', optimizer='adam')

# we don't compile the generator, because it's only trained through the full GAN model
codings_size = 100

# similar as autoencoder: 784 --> 150 --> 100 --> 150 --> 784
generator = Sequential()

generator.add(Dense(100, activation='relu', input_shape=[codings_size]))
generator.add(Dense(150, activation='relu'))
generator.add(Dense(784, activation='relu'))

generator.add(Reshape([28,28]))

GAN = Sequential([generator, discriminator])

# the discriminator shouldn't be trained during the second phase
discriminator.trainable = False

# layers
discriminator.layers[0].trainable

# hidden layer
discriminator.layers[1].trainable

GAN.compile(loss='binary_crossentropy', optimizer='adam')

# two sequential models
GAN.layers

GAN.summary()

# generator
GAN.layers[0].summary()

# discriminator
GAN.layers[1].summary()

"""### Setting up Training Batches"""

batch_size = 32

"""https://stackoverflow.com/questions/46444018/meaning-of-buffer-size-in-dataset-map-dataset-prefetch-and-dataset-shuffle

The buffer_size in Dataset.shuffle() can affect the randomness of your dataset, and hence the order in which elements are produced. 
"""

# change data, if you want
# my_data = X_train
my_data = only_zeros

dataset = tf.data.Dataset.from_tensor_slices(my_data).shuffle(buffer_size=1000)

type(dataset)

my_data.shape

# drop_remainder=True, because 5923/32 != 0
dataset = dataset.batch(batch_size, drop_remainder=True).prefetch(1)

epochs = 1

"""**NOTE: The generator never actually sees any real images. It learns by viewing the gradients going back through the discriminator. The better the discrimnator gets through training, the more information the discriminator contains in its gradients, which means the generator can being to make progress in learning how to generate fake images, in our case, fake zeros.**

## Training Loop
"""

# Grab the seprate components
generator, discriminator = GAN.layers

# For every epoch
for epoch in range(epochs):
  print