# -*- coding: utf-8 -*-
"""04-Keras-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/180V_SW1Jt6RWvGInrxMP6HfsFF0hMCo9

# Keras API Project Exercise

## The Data

We will be using a subset of the LendingClub DataSet obtained from Kaggle: https://www.kaggle.com/wordsforthewise/lending-club

## NOTE: Do not download the full zip from the link! We provide a special version of this file that has some extra feature engineering for you to do. You won't be able to follow along with the original file!

LendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California.[3] It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.

### Our Goal

Given historical data on loans given out with information on whether or not the borrower defaulted (charge-off), can we build a model that can predict wether or nor a borrower will pay back their loan? This way in the future when we get a new potential customer we can assess whether or not they are likely to pay back the loan. Keep in mind classification metrics when evaluating the performance of your model!

The "loan_status" column contains our label.

### Data Overview

-----
There are many LendingClub data sets on Kaggle. Here is the information on this particular data set:

<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LoanStatNew</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>loan_amnt</td>
      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.</td>
    </tr>
    <tr>
      <th>1</th>
      <td>term</td>
      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>int_rate</td>
      <td>Interest Rate on the loan</td>
    </tr>
    <tr>
      <th>3</th>
      <td>installment</td>
      <td>The monthly payment owed by the borrower if the loan originates.</td>
    </tr>
    <tr>
      <th>4</th>
      <td>grade</td>
      <td>LC assigned loan grade</td>
    </tr>
    <tr>
      <th>5</th>
      <td>sub_grade</td>
      <td>LC assigned loan subgrade</td>
    </tr>
    <tr>
      <th>6</th>
      <td>emp_title</td>
      <td>The job title supplied by the Borrower when applying for the loan.*</td>
    </tr>
    <tr>
      <th>7</th>
      <td>emp_length</td>
      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.</td>
    </tr>
    <tr>
      <th>8</th>
      <td>home_ownership</td>
      <td>The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER</td>
    </tr>
    <tr>
      <th>9</th>
      <td>annual_inc</td>
      <td>The self-reported annual income provided by the borrower during registration.</td>
    </tr>
    <tr>
      <th>10</th>
      <td>verification_status</td>
      <td>Indicates if income was verified by LC, not verified, or if the income source was verified</td>
    </tr>
    <tr>
      <th>11</th>
      <td>issue_d</td>
      <td>The month which the loan was funded</td>
    </tr>
    <tr>
      <th>12</th>
      <td>loan_status</td>
      <td>Current status of the loan</td>
    </tr>
    <tr>
      <th>13</th>
      <td>purpose</td>
      <td>A category provided by the borrower for the loan request.</td>
    </tr>
    <tr>
      <th>14</th>
      <td>title</td>
      <td>The loan title provided by the borrower</td>
    </tr>
    <tr>
      <th>15</th>
      <td>zip_code</td>
      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.</td>
    </tr>
    <tr>
      <th>16</th>
      <td>addr_state</td>
      <td>The state provided by the borrower in the loan application</td>
    </tr>
    <tr>
      <th>17</th>
      <td>dti</td>
      <td>A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income.</td>
    </tr>
    <tr>
      <th>18</th>
      <td>earliest_cr_line</td>
      <td>The month the borrower's earliest reported credit line was opened</td>
    </tr>
    <tr>
      <th>19</th>
      <td>open_acc</td>
      <td>The number of open credit lines in the borrower's credit file.</td>
    </tr>
    <tr>
      <th>20</th>
      <td>pub_rec</td>
      <td>Number of derogatory public records</td>
    </tr>
    <tr>
      <th>21</th>
      <td>revol_bal</td>
      <td>Total credit revolving balance</td>
    </tr>
    <tr>
      <th>22</th>
      <td>revol_util</td>
      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.</td>
    </tr>
    <tr>
      <th>23</th>
      <td>total_acc</td>
      <td>The total number of credit lines currently in the borrower's credit file</td>
    </tr>
    <tr>
      <th>24</th>
      <td>initial_list_status</td>
      <td>The initial listing status of the loan. Possible values are – W, F</td>
    </tr>
    <tr>
      <th>25</th>
      <td>application_type</td>
      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers</td>
    </tr>
    <tr>
      <th>26</th>
      <td>mort_acc</td>
      <td>Number of mortgage accounts.</td>
    </tr>
    <tr>
      <th>27</th>
      <td>pub_rec_bankruptcies</td>
      <td>Number of public record bankruptcies</td>
    </tr>
  </tbody>
</table>

---
"""

import pandas as pd

data_info = pd.read_csv('lending_club_info.csv', index_col = 'LoanStatNew')

data_info.head()

# example
print(data_info.loc['revol_util']['Description'])

# function to return feature description
def feat_info(col_name):
  print(data_info.loc[col_name]['Description'])

feat_info('grade')

"""## Loading the data"""

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv('lending_club_loan_two.csv')

df.head()

df.info()

"""# Exploratory Data Analysis

**OVERALL GOAL: Get an understanding for which variables are important, view summary statistics, and visualize the data**
"""

# visualizing the target variable
plt.figure(figsize=(8,5))
sns.countplot(x = 'loan_status', data=df)
plt.show()

df.describe().transpose()

plt.figure(figsize=(12,6))
sns.histplot(x = 'loan_amnt', data=df)
plt.show()

# correlation
df.corr()

# correlation heatmap
plt.figure(figsize=(12,7))
sns.heatmap(df.corr(), annot = True, cmap='icefire')
plt.show()

"""Almost perfect correlation between 'installment' and 'loan_amnt'."""

feat_info('installment')

feat_info('loan_amnt')

plt.figure(figsize=(8,5))
sns.scatterplot(x = 'installment', y='loan_amnt', data = df)
plt.show()

plt.figure(figsize=(8,5))
sns.boxplot(x = 'loan_status', y='loan_amnt', data = df)
plt.show()

df.groupby('loan_status')['loan_amnt'].describe()

feat_info('grade')

feat_info('sub_grade')

sorted(df['grade'].unique())

sorted(df['sub_grade'].unique())

plt.figure(figsize=(8,5))
sns.countplot(x = 'grade', hue = 'loan_status', data=df, order=['A', 'B', 'C', 'D', 'E', 'F', 'G'])
plt.show()

order = sorted(df['sub_grade'].unique())
plt.figure(figsize=(14,6))
sns.countplot(x = 'sub_grade', hue = 'loan_status', data=df, order=order)
plt.show()

plt.figure(figsize=(12,5))
sns.countplot(x = 'sub_grade', data=df, order=order, palette='icefire')
plt.show()

# isolating F and G subgrades
f_and_g = df[(df['grade'] == 'G') | (df['grade'] == 'F')]
order_2 = sorted(f_and_g['sub_grade'].unique())

plt.figure(figsize=(12,5))
sns.countplot(x = 'sub_grade', hue = 'loan_status', data=f_and_g, order=order_2)
plt.show()

df['loan_status'].unique()

# dummy variable
df['loan_repaid'] = df['loan_status'].map({'Fully Paid': 1, 'Charged Off': 0})

df[['loan_repaid', 'loan_status']]

# correlation between the 'target variable' and others numeric features
df.corr()['loan_repaid'][:-1].sort_values().plot(kind='bar', figsize = (7,4))
plt.show()

"""---
# Section 2: Data PreProcessing

**Section Goals: Remove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.**
"""

df.head()

# length of the DataFrame
df.shape[0]

# missing valules per column
df.isnull().sum()

# missing values percentage of the total DF
(df.isnull().sum()/len(df)) * 100

"""**Examining 'emp_title' and 'emp_length':**"""

feat_info('emp_title')
print('\n')
feat_info('emp_length')

df['emp_title'].nunique()

df['emp_title'].value_counts()

"""**Realistically there are too many unique job titles to try to convert this to a dummy variable feature. Let's remove that emp_title column:**"""

# droping 'emp_title'
df = df.drop('emp_title', axis=1)

df['emp_length'].nunique()

sorted(df['emp_length'].dropna().unique())

emp_length_order = [ '< 1 year',
                    '1 year',
                    '2 years',
                    '3 years',
                    '4 years',
                    '5 years',
                    '6 years',
                    '7 years',
                    '8 years',
                    '9 years',
                    '10+ years']

plt.figure(figsize=(12,5))
sns.countplot(x = 'emp_length', data=df, order=emp_length_order)
plt.show()

plt.figure(figsize=(12,5))
sns.countplot(x = 'emp_length', data=df, order=emp_length_order, hue='loan_status')
plt.show()

emp_co = df[df['loan_status']=="Charged Off"].groupby("emp_length").count()['loan_status']
emp_co

emp_fp = df[df['loan_status']=="Fully Paid"].groupby("emp_length").count()['loan_status']
emp_fp

emp_total = df.groupby("emp_length").count()['loan_status']
emp_total

emp_len = emp_co / emp_total
emp_len

emp_len.plot(kind = 'bar', figsize=(8,5))
plt.show()

"""**Charge off rates are extremely similar across all employment lengths. Go ahead and drop the emp_length column. So there is no strong relationship between employment length and being charged off.**"""

df = df.drop('emp_length', axis = 1)

df.isnull().sum()

"""**Review vs Purpose columns.**"""

feat_info('purpose')
print('\n')
feat_info('title')

df['purpose'].unique()

df['title'].head(10)

"""**The title column is simply a string subcategory/description of the purpose column.**"""

df = df.drop('title', axis = 1)

"""---
**Find out what the mort_acc feature represents**
"""

feat_info('mort_acc')

df['mort_acc'].value_counts()

# Correlation with the mort_acc column
df.corr()['mort_acc'].sort_values()[:-1]

"""**We will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry.**"""

print('Mean of mort_acc column per total_acc')
df.groupby('total_acc')['mort_acc'].mean()

feat_info('total_acc')

"""**Let's fill in the missing mort_acc values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its total_acc value from the Series we created above.**"""

total_acc_avg = df.groupby('total_acc').mean()['mort_acc']

# example
total_acc_avg[3.0]

'''
    Accepts the total_acc and mort_acc values for the row.
    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value
    for the corresponding total_acc value for that row.
    
    total_acc_avg here should be a Series or dictionary containing the mapping of the
    groupby averages of mort_acc per total_acc values.
'''
# creating the fill function
def fill_mort_acc(total_acc,mort_acc):
  if np.isnan(mort_acc):
    return total_acc_avg[total_acc]
  else:
    return mort_acc

# fill NaN with the mean
df['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis = 1)

df.isnull().sum()

"""***revol_util* and the *pub_rec_bankruptcies* have missing data points, but they account for less than 0.5% of the total data. So we remove the rows that are missing those values in those columns with dropna().**"""

# removing missing values
df = df.dropna()

df.isnull().sum()

len(df)

"""## Categorical Variables and Dummy Variables

**We're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.**

**TASK: List all the columns that are currently non-numeric. [Helpful Link](https://stackoverflow.com/questions/22470690/get-list-of-pandas-dataframe-columns-based-on-data-type)**

[Another very useful method call](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html)
"""

df.select_dtypes(include='object').columns

"""---
**Let's now go through all the string features to see what we should do with them.**

---

### term feature

**Convert the term feature into either a 36 or 60 integer numeric data type using .apply() or .map().**
"""

feat_info('term')

df['term'].unique()

df['term'] = df['term'].map({' 36 months': 36, ' 60 months': 60})
# df['term'] = df['term'].apply(lambda term: int(term[:3]))

"""### grade feature

**We already know grade is part of sub_grade, so just drop the grade feature.**
"""

df = df.drop('grade', axis = 1)

"""**TASK: Convert the subgrade into dummy variables. Then concatenate these new columns to the original dataframe. Remember to drop the original subgrade column and to add drop_first=True to your get_dummies call.**"""

subgrade_dummies = pd.get_dummies(df['sub_grade'], drop_first=True)

df = pd.concat([df.drop('sub_grade', axis = 1), subgrade_dummies], axis=1)

df.columns

df.select_dtypes(include='object').columns

"""### verification_status, application_type, initial_list_status, purpose 
**TASK: Convert these columns: ['verification_status', 'application_type','initial_list_status','purpose'] into dummy variables and concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.**
"""

dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose']],drop_first=True)
df = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)
df = pd.concat([df,dummies],axis=1)
# df.columns

"""### home_ownership
**TASK:Review the value_counts for the home_ownership column.**
"""

df['home_ownership'].value_counts()

"""**TASK: Convert these to dummy variables, but [replace](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.replace.html) NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER. Then concatenate them with the original dataframe. Remember to set drop_first=True and to drop the original columns.**"""

df = df.replace({'home_ownership': {'NONE': 'OTHER', 'ANY': 'OTHER'}})

dummies = pd.get_dummies(df['home_ownership'], drop_first=True)
df = pd.concat([df.drop('home_ownership', axis = 1), dummies], axis = 1)

"""### address
**TASK: Let's feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column.**
"""

# zip code = last 5 numbers
df['address']

# last 5 numbers
df['zip_code'] = df['address'].apply(lambda address: address[-5:])

"""**Now make this zip_code column into dummy variables using pandas. Concatenate the result and drop the original zip_code column along with dropping the address column.**"""

dummies = pd.get_dummies(df['zip_code'], drop_first=True)
df = df.drop(['zip_code', 'address'], axis=1)
df = pd.concat([df, dummies], axis=1)

# df.columns

df.select_dtypes('object').columns

"""### issue_d 

**TASK: This would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, drop this feature.**
"""

df['issue_d'].nunique()

feat_info('issue_d')

df = df.drop('issue_d', axis=1)

"""### earliest_cr_line
**TASK: This appears to be a historical time stamp feature. Extract the year from this feature using a .apply function, then convert it to a numeric feature. Set this new data to a feature column called 'earliest_cr_year'.Then drop the earliest_cr_line feature.**
"""

feat_info('earliest_cr_line')

df['earliest_cr_line']

df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda year: int(year[-4:]))
df = df.drop('earliest_cr_line', axis=1)

df.select_dtypes('object').columns

"""## Train Test Split"""

