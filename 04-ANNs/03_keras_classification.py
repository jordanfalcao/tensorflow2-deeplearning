# -*- coding: utf-8 -*-
"""03-Keras-Classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VxHI3-nEdRkfL5yNk3AxhzypXDn0ppay

# Keras TF 2.0 - Code Along Classification Project

Let's explore a classification task with Keras API for TF 2.0

## The Data

### Breast cancer wisconsin (diagnostic) dataset
--------------------------------------------

**Data Set Characteristics:**

    :Number of Instances: 569

    :Number of Attributes: 30 numeric, predictive attributes and the class

    :Attribute Information:
        - radius (mean of distances from center to points on the perimeter)
        - texture (standard deviation of gray-scale values)
        - perimeter
        - area
        - smoothness (local variation in radius lengths)
        - compactness (perimeter^2 / area - 1.0)
        - concavity (severity of concave portions of the contour)
        - concave points (number of concave portions of the contour)
        - symmetry 
        - fractal dimension ("coastline approximation" - 1)

        The mean, standard error, and "worst" or largest (mean of the three
        largest values) of these features were computed for each image,
        resulting in 30 features.  For instance, field 3 is Mean Radius, field
        13 is Radius SE, field 23 is Worst Radius.

        - class:
                - WDBC-Malignant
                - WDBC-Benign

    :Summary Statistics:

    ===================================== ====== ======
                                           Min    Max
    ===================================== ====== ======
    radius (mean):                        6.981  28.11
    texture (mean):                       9.71   39.28
    perimeter (mean):                     43.79  188.5
    area (mean):                          143.5  2501.0
    smoothness (mean):                    0.053  0.163
    compactness (mean):                   0.019  0.345
    concavity (mean):                     0.0    0.427
    concave points (mean):                0.0    0.201
    symmetry (mean):                      0.106  0.304
    fractal dimension (mean):             0.05   0.097
    radius (standard error):              0.112  2.873
    texture (standard error):             0.36   4.885
    perimeter (standard error):           0.757  21.98
    area (standard error):                6.802  542.2
    smoothness (standard error):          0.002  0.031
    compactness (standard error):         0.002  0.135
    concavity (standard error):           0.0    0.396
    concave points (standard error):      0.0    0.053
    symmetry (standard error):            0.008  0.079
    fractal dimension (standard error):   0.001  0.03
    radius (worst):                       7.93   36.04
    texture (worst):                      12.02  49.54
    perimeter (worst):                    50.41  251.2
    area (worst):                         185.2  4254.0
    smoothness (worst):                   0.071  0.223
    compactness (worst):                  0.027  1.058
    concavity (worst):                    0.0    1.252
    concave points (worst):               0.0    0.291
    symmetry (worst):                     0.156  0.664
    fractal dimension (worst):            0.055  0.208
    ===================================== ====== ======

    :Missing Attribute Values: None

    :Class Distribution: 212 - Malignant, 357 - Benign

    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian

    :Donor: Nick Street

    :Date: November, 1995

This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.
https://goo.gl/U2Uwz2

Features are computed from a digitized image of a fine needle
aspirate (FNA) of a breast mass.  They describe
characteristics of the cell nuclei present in the image.

Separating plane described above was obtained using
Multisurface Method-Tree (MSM-T) [K. P. Bennett, "Decision Tree
Construction Via Linear Programming." Proceedings of the 4th
Midwest Artificial Intelligence and Cognitive Science Society,
pp. 97-101, 1992], a classification method which uses linear
programming to construct a decision tree.  Relevant features
were selected using an exhaustive search in the space of 1-4
features and 1-3 separating planes.

The actual linear program used to obtain the separating plane
in the 3-dimensional space is that described in:
[K. P. Bennett and O. L. Mangasarian: "Robust Linear
Programming Discrimination of Two Linearly Inseparable Sets",
Optimization Methods and Software 1, 1992, 23-34].

This database is also available through the UW CS ftp server:

ftp ftp.cs.wisc.edu
cd math-prog/cpo-dataset/machine-learn/WDBC/

.. topic:: References

   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction 
     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on 
     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,
     San Jose, CA, 1993.
   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and 
     prognosis via linear programming. Operations Research, 43(4), pages 570-577, 
     July-August 1995.
   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques
     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) 
     163-171.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('cancer_classification.csv')

df.head()

df.info()

df.describe().transpose()

plt.figure(figsize=(8,5))
sns.countplot(x='benign_0__mal_1', data = df)
plt.show()

# correlation
df.corr()['benign_0__mal_1'].sort_values()

# correlation without benign_0__mal_1
plt.figure(figsize=(8,5))
df.corr()['benign_0__mal_1'][:-1].sort_values().plot(kind='bar')
plt.show()

# correlation heatmap
plt.figure(figsize=(10,10))
sns.heatmap(df.corr())
plt.show()

"""## Train Test Split"""

from sklearn.model_selection import train_test_split

X = df.drop('benign_0__mal_1', axis = 1).values
y = df['benign_0__mal_1'].values

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)

"""## Scaling Data"""

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

# training and transforming
X_train = scaler.fit_transform(X_train)

# just transforming
X_test = scaler.transform(X_test)

"""## Creating the Model

    # For a binary classification problem
    model.compile(optimizer='rmsprop',
                  loss='binary_crossentropy',
                  metrics=['accuracy'])
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation, Dropout

X_train.shape

model = Sequential()

model.add(Dense(units = 30, activation='relu'))
model.add(Dense(units = 15, activation='relu'))

# output layer with sigmoid activate fuction, better to classification problem
model.add(Dense(units = 1, activation='sigmoid'))

# For a binary classification problem
model.compile(loss='binary_crossentropy', optimizer='adam')

"""## Training the Model 

### Example One: Choosing too many epochs and overfitting!
"""

# https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network
# https://datascience.stackexchange.com/questions/18414/are-there-any-rules-for-choosing-the-size-of-a-mini-batch

model.fit(x=X_train, y=y_train,
          epochs=600,
          validation_data=(X_test,y_test), verbose = 1)

# model.history.history

model_loss = pd.DataFrame(model.history.history)

model_loss.head()

model_loss.plot(figsize = (8,5))
plt.show()

"""## Example Two: Early Stopping

We obviously trained too much! Let's use early stopping to track the val_loss and stop training once it begins increasing too much!
"""

# recreating the model
model = Sequential()

model.add(Dense(units=30,activation='relu'))
model.add(Dense(units=15,activation='relu'))

model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam')

from tensorflow.keras.callbacks import EarlyStopping

"""Stop training when a monitored quantity has stopped improving.

    Arguments:
        monitor: Quantity to be monitored.
        min_delta: Minimum change in the monitored quantity
            to qualify as an improvement, i.e. an absolute
            change of less than min_delta, will count as no
            improvement.
        patience: Number of epochs with no improvement
            after which training will be stopped.
        verbose: verbosity mode.
        mode: One of `{"auto", "min", "max"}`. In `min` mode,
            training will stop when the quantity
            monitored has stopped decreasing; in `max`
            mode it will stop when the quantity
            monitored has stopped increasing; in `auto`
            mode, the direction is automatically inferred
            from the name of the monitored quantity.
"""

# creating early stop parameter
early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)

# training again with early stop parameter
model.fit(x=X_train, y=y_train,
          epochs=600,
          validation_data=(X_test,y_test), verbose = 1,
          callbacks=[early_stop])

model_loss = pd.DataFrame(model.history.history)
model_loss.plot(figsize=(8,5))
plt.show()

"""## Example Three: Adding in DropOut Layers"""

from tensorflow.keras.layers import Dropout

# recreating the model
model = Sequential()

model.add(Dense(units=30,activation='relu'))
model.add(Dropout(0.5)) # 50% chance to dropout or turnoff a neuron 

model.add(Dense(units=15,activation='relu'))
model.add(Dropout(0.5))

model.add(Dense(units=1,activation='sigmoid'))

model.compile(loss='binary_crossentropy', optimizer='adam')

# training with early stop parameter again
model.fit(x=X_train, y=y_train,
          epochs=600,
          validation_data=(X_test,y_test), verbose = 1,
          callbacks=[early_stop])

model_loss = pd.DataFrame(model.history.history)
model_loss.plot(figsize=(8,5))
plt.show()

"""# Model Evaluation"""

# classification prediction
predictions = model.predict_classes(X_test)

# testing predictions in another way
predictions_2 = (model.predict(X_test) > 0.5).astype("int32")

from sklearn.metrics import classification_report, confusion_matrix

# https://en.wikipedia.org/wiki/Precision_and_recall
print(classification_report(y_test,predictions))

# we only get 1 False Negative and 3 False Positive
print(confusion_matrix(y_test, predictions))

# https://en.wikipedia.org/wiki/Precision_and_recall
print(classification_report(y_test,predictions_2))

# we only get 1 False Negative and 3 False Positive
print(confusion_matrix(y_test, predictions_2))