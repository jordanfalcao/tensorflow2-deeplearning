# -*- coding: utf-8 -*-
"""02-Keras-Regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Qhrx6eTanTcQyA9lLtLCtchgkreqceKz

# Keras Regression Code Along Project 

Let's now apply our knowledge to a more realistic data set. Here we will also focus on feature engineering and cleaning our data!

## The Data

We will be using data from a Kaggle data set:

https://www.kaggle.com/harlfoxem/housesalesprediction

#### Feature Columns
    
* id - Unique ID for each home sold
* date - Date of the home sale
* price - Price of each home sold
* bedrooms - Number of bedrooms
* bathrooms - Number of bathrooms, where .5 accounts for a room with a toilet but no shower
* sqft_living - Square footage of the apartments interior living space
* sqft_lot - Square footage of the land space
* floors - Number of floors
* waterfront - A dummy variable for whether the apartment was overlooking the waterfront or not
* view - An index from 0 to 4 of how good the view of the property was
* condition - An index from 1 to 5 on the condition of the apartment,
* grade - An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design.
* sqft_above - The square footage of the interior housing space that is above ground level
* sqft_basement - The square footage of the interior housing space that is below ground level
* yr_built - The year the house was initially built
* yr_renovated - The year of the house’s last renovation
* zipcode - What zipcode area the house is in
* lat - Lattitude
* long - Longitude
* sqft_living15 - The square footage of interior housing living space for the nearest 15 neighbors
* sqft_lot15 - The square footage of the land lots of the nearest 15 neighbors
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv('kc_house_data.csv')

df.head()

"""# Exploratory Data Analysis"""

# null verification
df.isnull().sum()

# data description
df.describe().transpose()

# label histogram
plt.figure(figsize=(12,6))
plt.title('Price Histogram', fontsize=20)
plt.xlabel('Price - U$D', fontsize=16)
plt.ylabel('Count', fontsize=16)
sns.histplot(df['price']);

# bedrooms countplot
plt.figure(figsize=(12,6))
ax = sns.countplot(x = df['bedrooms'])
ax.axes.set_title("Number of bedrooms", fontsize=20)
ax.set_ylabel("Count", fontsize=16)
ax.set_xlabel("Bedrooms", fontsize=16)
plt.show()

# label histogram
plt.figure(figsize=(12,6))
ax = sns.boxplot(x = 'bedrooms', y = 'price', data = df)
ax.axes.set_title("Number of bedrooms", fontsize=20)
ax.set_ylabel("Price", fontsize=16)
ax.set_xlabel("Bedrooms", fontsize=16)
plt.show()

# correlation
df.corr()['price'].sort_values()

"""## Scatterplot with high correlation features"""

# correlation between price and interior squared footage
plt.figure(figsize=(12,6))
ax = sns.scatterplot(x = 'price', y = 'sqft_living', data = df)
ax.axes.set_title("Correlation between price and interior squared footage", fontsize=20)
ax.set_ylabel("Interior square footage", fontsize=16)
ax.set_xlabel("Price", fontsize=16)
plt.show()

# correlation between price and apartament design
plt.figure(figsize=(12,6))
ax = sns.scatterplot(y = 'price', x = 'grade', data = df)
ax.axes.set_title("Correlation between price and apartament design", fontsize=20)
ax.set_ylabel("Price", fontsize=16)
ax.set_xlabel("Design Grade", fontsize=16)
plt.show()

"""### Geographical Properties"""

# preço por longitude
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='long',data=df)
plt.show()

# preço por latitude
plt.figure(figsize=(12,8))
sns.scatterplot(x='price',y='lat',data=df)
plt.show()

# visualizando a área com maior preço
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=df,hue='price')
plt.show()

"""* Observamos que a área próxima a (-122.2, 47.6) possui altos valores.

### Removing some outliers
"""

# 20 most expensives houses
df.sort_values('price', ascending=False).head(20)

# DF 1%
len(df)*(0.01)

# creating another DF without 1% most expensives houses
non_top_1_perc = df.sort_values('price', ascending=False).iloc[216:]

# visualizando a área com maior preço no DF sem 1% mais caro
plt.figure(figsize=(12,8))
sns.scatterplot(x='long',y='lat',data=non_top_1_perc,
                edgecolor = None, alpha=0.2, palette = 'RdYlGn', hue='price')
plt.show()

# waterfront houses
plt.figure(figsize = (8,5))
sns.boxplot(x='waterfront',y='price',data=df)
plt.show()

"""## Working with Feature Data"""

df.info()

df = df.drop('id', axis=1)

df.head()

"""### Feature Engineering from Date"""

# converting to datetime
df['date'] = pd.to_datetime(df['date'])

df['date']

# extracting year
df['month'] = df['date'].apply(lambda date: date.month)

# extracting year
df['year'] = df['date'].apply(lambda date: date.year)

plt.figure(figsize=(12,6))
sns.boxplot(x='month', y = 'price', data = df)
plt.show()

# easier way to see the information
df.groupby('month').mean()['price'].plot()

df.groupby('year').mean()['price'].plot()

# drop date
df = df.drop('date', axis=1)

df.columns

# there are 70 categories
df['zipcode'].value_counts()

df = df.drop('zipcode', axis=1)

df.columns

# most year is 0
df['yr_renovated'].value_counts()

# it also makes sense, 0 = no basement
df['sqft_basement'].value_counts()

"""## Scaling and Train Test Split"""

X = df.drop('price', axis=1).values
y = df['price'].values

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)

"""### Scaling"""

from sklearn.preprocessing import MinMaxScaler

# initializing
scaler = MinMaxScaler()

# training and transforming
X_train = scaler.fit_transform(X_train)

# just transforming
X_test = scaler.transform(X_test)

X_train.shape

X_test.shape

"""## Creating a Model"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam

# creating
model = Sequential()

# add hiden layers one by one
model.add(Dense(19, activation='relu'))
model.add(Dense(19, activation='relu'))
model.add(Dense(19, activation='relu'))
model.add(Dense(19, activation='relu'))
model.add(Dense(1))

model.compile(optimizer='adam', loss='mse')

"""## Training the Model"""

# passing validation data, not to update the weights and bias, just to validation
model.fit(x=X_train, y=y_train,
          validation_data=(X_test,y_test),        # .values
          batch_size=128, epochs=400) # the smaller the batch, the longer the training

# loss and validation loss
losses = pd.DataFrame(model.history.history)
losses.head()

# comparing train and test loss 
losses.plot(figsize=(8,5))
plt.xlim(0,400);

"""Our validation loss continues decreasing, so there is nos overfiting and we can keep training our data.

# Evaluation on Test Data

https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score

predictions = model.predict(X_test)

# root mean square error
np.sqrt(mean_squared_error(y_test, predictions))

# mean absolute error
mean_absolute_error(y_test, predictions)

# best possible score is 1.0
explained_variance_score(y_test,predictions)

# mean of the price to compare
df['price'].mean()

"""Our mean absolute error and root mean square error are not great in comparison with mean of the price. """

# comparing
plt.figure(figsize=(12,6))

# perfect predictions
plt.plot(y_test,y_test,'r')

# predictions
plt.scatter(y_test, predictions)
plt.show()

errors = y_test.reshape(6480, 1) - predictions

plt.figure(figsize=(10,6))
sns.histplot(errors)
plt.show()

"""### Predicting on a brand new house"""

# features
df.drop('price', axis=1).iloc[0]

single_house = df.drop('price', axis=1).iloc[0]

# scaling
single_house = scaler.transform(single_house.values.reshape(-1, 19))

model.predict(single_house)

df['price'].head(1)

"""Quite different the prediction from the real.
We can remove 1% or 2% most expensive houses from the dataset.
"""